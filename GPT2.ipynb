{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdZBUbXNqbWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules import ModuleList\n",
        "from torch.nn.modules.normalization import LayerNorm\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm_notebook, trange\n",
        "import logging\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "logger = logging.getLogger()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB2de2J70Zti",
        "colab_type": "text"
      },
      "source": [
        "### Conv1D Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPFAgTOtqf4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nx, nf):\n",
        "        super().__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02) # Fills the input Tensor with values drawn from the normal distribution\n",
        "        self.weight = nn.Parameter(w) #----> shape = (nx, nf)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf)) #----> shape = (nf)\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,) #----> shape = (all except last dimension of input 'x', concatenated with nf)\n",
        "        # Multiply input with weights and add bias to replicate a forward pass\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight) #----> converts input 'x' into, (y x last dimension of the x itself)\n",
        "        x = x.view(*size_out)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ta6h7zlttBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ce324e6-fd9b-49bc-b713-7944f5ccdacf"
      },
      "source": [
        "d_model = 768\n",
        "conv1d  = Conv1D(d_model, d_model*3) #3 because, 1 for Query, 1 for Key, 1 for Value respectively\n",
        "x       = torch.rand(1, 4, d_model) # represents a sequence of batch_size=1, seq_len=4 and embedding_sz=768, something like \"Hello how are you\"\n",
        "x       = conv1d(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 2304])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQEf2tlWzWhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Verifying add part of x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "# a = torch.rand(1, 2034)\n",
        "# b = torch.rand(4, 2034)\n",
        "# print(a)\n",
        "# print(b)\n",
        "# print((a+b))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAMb-m4h0i6s",
        "colab_type": "text"
      },
      "source": [
        "### Feedforward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GQi1OKM0Op-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
        "        super().__init__()\n",
        "        self.c_fc    = Conv1D(d_model, nx)\n",
        "        self.c_proj  = Conv1D(nx, d_model)\n",
        "        self.act     = F.gelu\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaS7q9fI17wb",
        "colab_type": "text"
      },
      "source": [
        "### Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9uixtqY1-32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n",
        "        super().__init__()\n",
        "        self.n_head  = n_head\n",
        "        self.d_model = d_model\n",
        "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
        "        self.scale   = scale\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.c_proj  = Conv1D(d_model, d_model)\n",
        "        \n",
        "    def split_heads(self, x):\n",
        "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
        "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) #----> shape = (1, 1, 12, 192) because 2304//12 = 192\n",
        "        x = x.view(*new_shape)\n",
        "        return x.permute(0, 2, 1, 3) #----> shape = (1, 12, 1, 192)\n",
        "    \n",
        "    def _attn(self, q, k, v, attn_mask=None):\n",
        "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
        "        if self.scale: \n",
        "          scores = scores/math.sqrt(v.size(-1))\n",
        "        nd, ns  = scores.size(-2), scores.size(-1)\n",
        "        if attn_mask is not None: \n",
        "          scores = scores + attn_mask\n",
        "        scores  = self.softmax(scores)\n",
        "        scores  = self.dropout(scores)\n",
        "        outputs = torch.matmul(scores, v)\n",
        "        return outputs\n",
        "    \n",
        "    def merge_heads(self, x):\n",
        "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),) \n",
        "        return x.view(*new_shape)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x        = self.c_attn(x) # new `x` shape - `[1,3,2304]`\n",
        "        q, k, v  = x.split(self.d_model, dim=2) # splits along second dimension, each of q, k and v is of the shape (1, 1, 2304)\n",
        "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "        out      = self._attn(q, k, v)\n",
        "        out      = self.merge_heads(out)\n",
        "        out      = self.c_proj(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1zrYBY_yHu",
        "colab_type": "text"
      },
      "source": [
        "### Transformer Block (GPT uses only Decoder whereas BERT uses only Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cECC41zU_ys6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n",
        "        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n",
        "        self.ln_1        = LayerNorm(d_model)\n",
        "        self.ln_2        = LayerNorm(d_model)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.feedforward(self.ln_2(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr7ayWQU_95v",
        "colab_type": "text"
      },
      "source": [
        "### GPT2 Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoWKnpFeACao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_clones(module, n):\n",
        "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.nlayers = nlayers\n",
        "        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n",
        "        self.h       = _get_clones(block, 12)\n",
        "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
        "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
        "        self.drop    = nn.Dropout(0.1)\n",
        "        self.ln_f    = LayerNorm(d_model)\n",
        "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        self.out.weight = self.wte.weight\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "    \n",
        "    def forward(self, src, labels=None, pos_ids=None):\n",
        "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
        "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
        "        for i in range(self.nlayers): inp = self.h[i](inp)\n",
        "        inp     = self.ln_f(inp)\n",
        "        logits  = self.out(inp)\n",
        "        outputs = (logits,) + (inp,)\n",
        "        \n",
        "        if labels is not None:\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "            return outputs\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uwm78cdj0LB",
        "colab_type": "text"
      },
      "source": [
        "### Load the Pretrained Weights from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut5Z7TREjrED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40452293-02af-48df-9141-66bebf912deb"
      },
      "source": [
        "model = GPT2()\n",
        "# load pretrained_weights from hugging face\n",
        "# download file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin to `.`\n",
        "\n",
        "model_dict = model.state_dict() #currently with random initialization\n",
        "state_dict = torch.load(\"/content/drive/My Drive/gpt2-pytorch_model.bin\") #pretrained weights\n",
        "\n",
        "old_keys = []\n",
        "new_keys = []\n",
        "for key in state_dict.keys(): \n",
        "    if \"mlp\" in key: #The hugging face state dict references the feedforward network as mlp, need to replace to `feedforward` be able to reuse these weights\n",
        "        new_key = key.replace(\"mlp\", \"feedforward\")\n",
        "        new_keys.append(new_key)\n",
        "        old_keys.append(key)\n",
        "\n",
        "for old_key, new_key in zip(old_keys, new_keys): \n",
        "    state_dict[new_key]=state_dict.pop(old_key)\n",
        "\n",
        "pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n",
        "\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval() #model in inference mode as it's now initialized with pretrained weights"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2(\n",
              "  (h): ModuleList(\n",
              "    (0): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attn): Attention(\n",
              "        (c_attn): Conv1D()\n",
              "        (softmax): Softmax(dim=-1)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (c_proj): Conv1D()\n",
              "      )\n",
              "      (feedforward): FeedForward(\n",
              "        (c_fc): Conv1D()\n",
              "        (c_proj): Conv1D()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (wte): Embedding(50257, 768)\n",
              "  (wpe): Embedding(1024, 768)\n",
              "  (drop): Dropout(p=0.1, inplace=False)\n",
              "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (out): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  (loss_fn): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nSnw1a0j-HV",
        "colab_type": "text"
      },
      "source": [
        "### Use Hugging Face's Tokenizer and performing predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyMl5z0ojynl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "697ba827-0488-404d-924f-4e5acf198adf"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "context   = torch.tensor([tokenizer.encode(\"The planet earth\")])\n",
        "\n",
        "\n",
        "def generate(context, ntok=20):\n",
        "    for _ in range(ntok):\n",
        "        out = model(context)\n",
        "        logits = out[:, -1, :]\n",
        "        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = np.NINF\n",
        "        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n",
        "        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n",
        "    return context\n",
        "\n",
        "out = generate(context, ntok=20)\n",
        "tokenizer.decode(out[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.5.0+cu101 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The planet earth is a great and a different place again. place and there were people who are you're there.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn46FOe3sAQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01700f26-0470-42ec-ef65-d2edf5089bd8"
      },
      "source": [
        "context = torch.tensor([tokenizer.encode(\"The President\")])\n",
        "out = generate(context, ntok=30)\n",
        "tokenizer.decode(out[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The President of the Church of Islam and the Muslim of the Prophet.\"\\n\\n\\n\\n\"\\n\\n\\n of the president\\'s in\". I\".\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoRUQ6eSsT3O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3aa7e779-610a-4418-e4d6-9074604ac8cb"
      },
      "source": [
        "context = torch.tensor([tokenizer.encode(\"United States is\")])\n",
        "out = generate(context, ntok=30)\n",
        "tokenizer.decode(out[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'United States is looking for a little while.\\n\\n\\n\\nA. to for\\n# ( for ( for\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWQPjxlYsfR3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b634575-c97a-4bf6-f015-87ce9ff610c1"
      },
      "source": [
        "context = torch.tensor([tokenizer.encode(\"He is a\")])\n",
        "out = generate(context, ntok=30)\n",
        "tokenizer.decode(out[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He is a great guy to know to keep a great man, great guy who we don\\'t you?\" he has a good guy who are the great.\" I the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48CK86xXsjNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2f20158-6185-46fb-c338-7254abade82d"
      },
      "source": [
        "context = torch.tensor([tokenizer.encode(\"She is a\")])\n",
        "out = generate(context, ntok=30)\n",
        "tokenizer.decode(out[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She is a real person with a good person with some real body with real life,\" some real. good. \"some. not an\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}